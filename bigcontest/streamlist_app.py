import streamlit as st
import google.generativeai as genai
import json
import os
import time

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Gemini API ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.warning("âš ï¸ í™˜ê²½ë³€ìˆ˜ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit secretsì— í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
else:
    genai.configure(api_key=GOOGLE_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. personas.json ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_personas(file_path="personas.json"):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            personas = json.load(f)
        return personas
    except FileNotFoundError:
        st.error(f"âŒ personas.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `prompt_generator.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return []
    except json.JSONDecodeError:
        st.error("âŒ personas.json íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return []

personas = load_personas()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Streamlit UI êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="AI ë§ˆì¼€íŒ… ì „ëµ ì»¨ì„¤í„´íŠ¸ (Gemini)", layout="wide")
st.title("ğŸ’¡ AI ë§ˆì¼€íŒ… ì „ëµ ì»¨ì„¤í„´íŠ¸")

if not personas:
    st.stop()

# JSON ë°ì´í„°ì—ì„œ ê³ ìœ ê°’ ì¶”ì¶œ
industries = sorted({p["ì—…ì¢…"] for p in personas})
franchise_types = sorted({p["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"] for p in personas})
store_ages = sorted({p["ì í¬ì—°ë ¹"] for p in personas})
age_groups = sorted({p["ê³ ê°ì—°ë ¹ëŒ€"] for p in personas})
behaviors = sorted({p["ê³ ê°í–‰ë™"] for p in personas})

col1, col2, col3 = st.columns(3)
industry = col1.selectbox("ì—…ì¢… ì„ íƒ", industries)
franchise = col2.selectbox("ì í¬ í˜•íƒœ", franchise_types)
store_age = col3.selectbox("ì í¬ ì—°ë ¹", store_ages)

col4, col5 = st.columns(2)
age_group = col4.selectbox("ê³ ê° ì—°ë ¹ëŒ€", age_groups)
behavior = col5.selectbox("ê³ ê° í–‰ë™ íŠ¹ì„±", behaviors)

generate_button = st.button("ğŸ” ì „ëµ ìƒì„±í•˜ê¸°")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Gemini API í˜¸ì¶œ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash
def call_gemini(prompt, model_name="gemini-2.5-flash", temperature=0.6, max_output_tokens=65535):
    """
    Stream partial Gemini responses so the UI can render them incrementally.
    """
    try:
        model = genai.GenerativeModel(model_name)
        generation_config = {"temperature": temperature, "max_output_tokens": max_output_tokens}
        responses = model.generate_content(
            prompt,
            generation_config=generation_config,
            stream=True,
        )

        for chunk in responses:
            # `chunk.text` contains the incremental delta for the chunk.
            text_chunk = getattr(chunk, "text", None)
            if not text_chunk and getattr(chunk, "candidates", None):
                candidate = chunk.candidates[0] if chunk.candidates else None
                if candidate and getattr(candidate, "content", None):
                    parts = getattr(candidate.content, "parts", [])
                    text_chunk = "".join(
                        part.text for part in parts if hasattr(part, "text")
                    )

            if text_chunk:
                yield text_chunk

        # Ensure the stream is fully resolved so later reads (if any) succeed.
        try:
            responses.resolve()
        except Exception:
            pass
    except Exception as e:
        yield f"âŒ Gemini í˜¸ì¶œ ì˜¤ë¥˜: {e}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ì„ íƒëœ persona â†’ prompt ë¶ˆëŸ¬ì˜¤ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_prompt(personas, industry, franchise, store_age, age_group, behavior):
    for p in personas:
        if (
            p["ì—…ì¢…"] == industry
            and p["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"] == franchise
            and p["ì í¬ì—°ë ¹"] == store_age
            and p["ê³ ê°ì—°ë ¹ëŒ€"] == age_group
            and p["ê³ ê°í–‰ë™"] == behavior
        ):
            return p["prompt"]
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if generate_button:
    selected_prompt = find_prompt(personas, industry, franchise, store_age, age_group, behavior)
    if not selected_prompt:
        st.error("âš ï¸ í•´ë‹¹ ì¡°í•©ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. prompt_generator.pyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    st.subheader("ğŸ§© ì„ íƒëœ í˜ë¥´ì†Œë‚˜")
    st.markdown(f"""
    - ì—…ì¢…: **{industry}**
    - í˜•íƒœ: **{franchise}**
    - ì í¬ ì—°ë ¹: **{store_age}**
    - ê³ ê° ì—°ë ¹ëŒ€: **{age_group}**
    - ê³ ê° í–‰ë™: **{behavior}**
    """)

    with st.expander("ğŸ“œ í”„ë¡¬í”„íŠ¸ ë³´ê¸°"):
        st.code(selected_prompt, language="markdown")

    st.markdown("### ğŸ“ˆ ìƒì„±ëœ ë§ˆì¼€íŒ… ì „ëµ ê²°ê³¼")
    status_placeholder = st.empty()
    status_placeholder.info("ì „ëµì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤... â³")

    status_messages = [
        "1/4 ì‹œì¥ ë° ê²½ìŸ ë°ì´í„°ë¥¼ ê²€í† í•˜ê³  ìˆì–´ìš”...",
        "2/4 ë”± ë§ëŠ” ë§ˆì¼€íŒ… ì±„ë„ì„ ì¡°ì‚¬í•˜ê³ í•˜ê³  ìˆì–´ìš”...",
        "3/4 ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµ ì•„ì´ë””ì–´ë¥¼ ì¡°í•©í•˜ëŠ” ì¤‘ì´ì—ìš”...",
        "4/4 ì „ë‹¬í•  ë‚´ìš©ì„ ì •ëˆí•˜ê³  ìˆì–´ìš”...",
    ]
    step_interval = 2.0
    step_state = {"idx": 0, "next_time": time.time() + step_interval}

    def stream_with_status():
        for chunk in call_gemini(selected_prompt):
            now = time.time()
            if step_state["idx"] < len(status_messages) and now >= step_state["next_time"]:
                status_placeholder.info(
                    f"ì „ëµì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤... â³\n\n{status_messages[step_state['idx']]}"
                )
                step_state["idx"] += 1
                step_state["next_time"] = now + step_interval
            yield chunk

    result = st.write_stream(stream_with_status())

    if not result or not result.strip():
        status_placeholder.warning("âš ï¸ Geminië¡œë¶€í„° ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        st.warning("âš ï¸ Geminië¡œë¶€í„° ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    elif result.lstrip().startswith("âŒ"):
        status_placeholder.error(result)
        st.error(result)
    else:
        status_placeholder.success("ì „ëµ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! âœ…")
        st.download_button(
            label="â¬‡ï¸ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Markdown)",
            data=result,
            file_name=f"marketing_plan_{industry}_{store_age}.md",
            mime="text/markdown",
        )
