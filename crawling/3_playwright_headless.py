# scrape_coupang_jobs.py
from playwright.sync_api import sync_playwright
from urllib.parse import urljoin

def scrape_data_jobs(max_pages=50):
    """
    Coupang Jobs 'data' ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    max_pages: ìˆœíšŒí•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ì•ˆì •ì„±ì„ ìœ„í•´ ì„¤ì •)
    """
    results = []
    base_url = 'https://www.coupang.jobs'
    search_path = '/en/jobs/?origin=global&search=data&page={}'

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        for page_num in range(1, max_pages + 1):
            url = base_url + search_path.format(page_num)
            print(f"ğŸ•¸  Fetching page {page_num}: {url}")
            page.goto(url, wait_until='networkidle')

            # gh_jid íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ë§í¬(<a>)ë§Œ ì¶”ì¶œ
            job_links = page.query_selector_all('a[href*="?gh_jid="]')
            if not job_links:
                print("ğŸ”š ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            for link in job_links:
                title = link.inner_text().strip()
                href = link.get_attribute('href')
                full_url = urljoin(base_url, href)

                # ê³µê³  ìš”ì†Œ ë°”ë¡œ ì•„ë˜ì— ìˆì„ ìˆ˜ ìˆëŠ” <p> ë˜ëŠ” <span>ì—ì„œ location ì¶”ì¶œ
                loc_el = link.query_selector('p, span')
                location = loc_el.inner_text().strip() if loc_el else ''

                results.append({
                    'title': title,
                    'url': full_url,
                    'location': location
                })

            print(f"âœ… Page {page_num}: {len(job_links)}ê°œ ìˆ˜ì§‘")
        
        browser.close()
    return results

if __name__ == '__main__':
    jobs = scrape_data_jobs(max_pages=100)
    print(f"\nì´ {len(jobs)}ê°œì˜ ê³µê³ ë¥¼ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤.\n")
    for idx, job in enumerate(jobs, 1):
        print(f"{idx:3d}. {job['title']} | {job['location']} | {job['url']}")
