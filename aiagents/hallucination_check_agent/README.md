# ğŸ¤– Hallucination Check Agent

![Agent Flow Diagram](assets/agent_flow.png)

![Pydantic AI](https://img.shields.io/badge/agent%20fw-pydantic_ai-pink.svg)
![Langfuse](https://img.shields.io/badge/observability-langfuse-red.svg)
![A2A Protocol](https://img.shields.io/badge/protocol-A2A-green.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

**Advanced NLI-based hallucination detection agent for LLM outputs**

LLM ì‘ë‹µ ë‚´ í• ë£¨ì‹œë„¤ì´ì…˜(ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ì •ë³´)ê³¼ ìê¸°ëª¨ìˆœ ì—¬ë¶€ë¥¼ ê²€ì¶œí•˜ëŠ” í‰ê°€ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. Natural Language Inference(NLI) ê¸°ë°˜ìœ¼ë¡œ AI ëª¨ë¸ ì‘ë‹µì˜ í• ë£¨ì‹œë„¤ì´ì…˜ì„ íƒì§€í•˜ëŠ” ê³ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œìœ¼ë¡œ, Agent-to-Agent(A2A) í”„ë¡œí† ì½œì„ í†µí•´ ì„œë¹„ìŠ¤ë©ë‹ˆë‹¤.

## âœ¨ Key Features

ğŸ” **Dual Evaluation Modes**
- **Search Mode**: Tavily ê²€ìƒ‰ì„ í†µí•œ ìë™ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í‰ê°€
- **Context Mode**: ì‚¬ìš©ì ì œê³µ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ì ‘ í‰ê°€

ğŸ¤– **Multi-Agent Architecture**  
- **GPT-4o**: ì •í™•í•œ NLI í‰ê°€ ë° í•œêµ­ì–´ ìš”ì•½
- **GPT-3.5-turbo**: íš¨ìœ¨ì ì¸ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘

ğŸŒ **A2A Protocol Integration**
- í‘œì¤€í™”ëœ Agent-to-Agent í”„ë¡œí† ì½œ ì§€ì›
- í™•ì¥ ê°€ëŠ¥í•œ í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

ğŸ“Š **Production Ready**
- Langfuse ê¸°ë°˜ ì™„ì „í•œ ê´€ì°° ê°€ëŠ¥ì„±
- ìë™ ì¬ì‹œë„ ë° ì•ˆì •ì„± ë³´ì¥
- ìƒì„¸í•œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë° ì¶”ì 

---

## ğŸ”§ Basic Concepts

### NLI ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¶œ

ì´ ì‹œìŠ¤í…œì€ **Natural Language Inference(NLI)** ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ í• ë£¨ì‹œë„¤ì´ì…˜ì„ ê²€ì¶œí•©ë‹ˆë‹¤:

1. **ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘**: Tavily ê²€ìƒ‰ ë˜ëŠ” ì‚¬ìš©ì ì œê³µ ì»¨í…ìŠ¤íŠ¸
2. **ì¼ê´€ì„± í‰ê°€**: ì»¨í…ìŠ¤íŠ¸ì™€ ê²€ì¦í•  ë¬¸ì¥ ê°„ì˜ ë…¼ë¦¬ì  ê´€ê³„ ë¶„ì„
3. **ì ìˆ˜ ì‚°ì¶œ**: 0.0(ì¼ì¹˜) ~ 1.0(ëª¨ìˆœ) ì‚¬ì´ì˜ í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜
4. **ì´ìœ  ì œê³µ**: í‰ê°€ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…

### ë‘ ê°€ì§€ í‰ê°€ ëª¨ë“œ

**ê²€ìƒ‰ ëª¨ë“œ (Search Mode)**:
- Tavily APIë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
- ì°¬ì„±(pros)/ë°˜ëŒ€(cons) ê´€ì  ì§€ì›
- ì¼ë°˜ì ì¸ ì‚¬ì‹¤ í™•ì¸ì— ì í•©

**ì»¨í…ìŠ¤íŠ¸ ëª¨ë“œ (Context Mode)**:
- ì‚¬ìš©ìê°€ ì§ì ‘ ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸ ì œê³µ
- ë„ë©”ì¸ë³„ íŠ¹í™” í‰ê°€ ê°€ëŠ¥
- ë” ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„

---

## ğŸ¤– Agent Types

### 1. Get Source Agent (GPT-3.5-turbo)
- **ì—­í• **: Tavily ê²€ìƒ‰ì„ í†µí•œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° ìš”ì•½
- **ì…ë ¥**: ê²€ì¦í•  ì§ˆì˜ + ê²€ìƒ‰ ê´€ì (pros/cons)
- **ì¶œë ¥**: ìš”ì•½ëœ ì»¨í…ìŠ¤íŠ¸ + ì°¸ì¡° URL ëª©ë¡

### 2. Context Consistency Agent (GPT-4o)  
- **ì—­í• **: NLI ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€
- **ì…ë ¥**: ì»¨í…ìŠ¤íŠ¸ + ê²€ì¦í•  ë¬¸ì¥
- **ì¶œë ¥**: í• ë£¨ì‹œë„¤ì´ì…˜ ì ìˆ˜(0.0-1.0) + í‰ê°€ ì´ìœ 

### 3. Reason Summary Agent (GPT-4o)
- **ì—­í• **: ë‹¤ì¤‘ í‰ê°€ ê²°ê³¼ì˜ í•œêµ­ì–´ ìš”ì•½
- **ì…ë ¥**: ì—¬ëŸ¬ í‰ê°€ ì´ìœ  ëª©ë¡
- **ì¶œë ¥**: í†µí•©ëœ í•œêµ­ì–´ ìš”ì•½ (300ì ì´ë‚´)

---

## ğŸ“ Project Structure

```
halucination_check_agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ client/                          #
â”‚   â”‚   â”œâ”€â”€ a2a_client.py               # CLI A2A í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â””â”€â”€ fasta2a_client.py           # FastA2A(pydantic-ai) í´ë¼ì´ì–¸íŠ¸
â”‚   â””â”€â”€ server/                         
â”‚       â”œâ”€â”€ agent/                      # ì—ì´ì „íŠ¸
â”‚       â”‚   â”œâ”€â”€ context_consistency_agent.py  # NLI ê¸°ë°˜ ì¼ê´€ì„± í‰ê°€
â”‚       â”‚   â”œâ”€â”€ get_source_agent.py           # ê²€ìƒ‰ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
â”‚       â”‚   â””â”€â”€ reason_summary_agent.py       # í‰ê°€ ê²°ê³¼ í•œêµ­ì–´ ìš”ì•½
â”‚       â”œâ”€â”€ app_a2a.py                  # A2A í”„ë¡œí† ì½œ ì„œë²„
â”‚       â”œâ”€â”€ app_fasta2a.py              # FastA2A(pydantic-ai) ì„œë²„
â”‚       â”œâ”€â”€ check_search_graph.py       # ê²€ìƒ‰ ê·¸ë˜í”„ ì‹¤í–‰ ë¡œì§
â”‚       â”œâ”€â”€ check_context_graph.py      # ì»¨í…ìŠ¤íŠ¸ ê·¸ë˜í”„ ì‹¤í–‰ ë¡œì§
â”‚       â”œâ”€â”€ executor.py                 # A2A ìš”ì²­ ì²˜ë¦¬ ì‹¤í–‰ê¸°
â”‚       â”œâ”€â”€ dto.py                      # ë°ì´í„° íƒ€ì… ê°ì²´
â”‚       â”œâ”€â”€ constant.py                 # ì„¤ì • ìƒìˆ˜
â”‚       â”œâ”€â”€ langfuse_trace.py           # Langfuse ì¶”ì  ì„¤ì •
â”‚       â””â”€â”€ util.py                     
â”œâ”€â”€ langfuse/                           # Langfuse ìŠ¤íƒ
â”‚   â”œâ”€â”€ docker-compose.yml              # Langfuse ë„ì»¤ ì„¤ì •
â”‚   â””â”€â”€ ...                             
â”œâ”€â”€ assets/                             
â”‚   â”œâ”€â”€ agent_flow.png                  
â”‚   â”œâ”€â”€ what_is_nli.md                  # NLI ê°œë… ì„¤ëª… ë¬¸ì„œ
â”‚   â””â”€â”€ example_context.txt             # ì˜ˆì‹œ ì»¨í…ìŠ¤íŠ¸
â”œâ”€â”€ state/                              # ê·¸ë˜í”„ ìƒíƒœ ì €ì¥ì†Œ(for human in the loop)
â”œâ”€â”€ .env.example                        # í™˜ê²½ë³€ìˆ˜ í…œí”Œë¦¿
â”œâ”€â”€ pyproject.toml                      
â”œâ”€â”€ uv.lock                            
â””â”€â”€ README.md                          
```
---

## ğŸš€ Quick Start

### 1. ğŸ“¦ Installation

```bash
git clone https://github.com/middlek/halucination_check_agent.git
cd halucination_check_agent
uv venv
source .venv/bin/activate
uv sync
```

### 2. ğŸ”§ Setup Observability

```bash
cd langfuse
docker compose up -d
```

> ğŸ“Œ Langfuse runs at `http://localhost:3000` - Create project and get API keys

### 3. âš™ï¸ Environment Configuration

```bash
# Copy and configure environment variables
cp .env.example .env

# Edit .env with your API keys:
# âœ… OpenAI API key (required)
# âœ… Tavily API key (required for search mode)  
# âœ… Langfuse keys (required for observability)
```

### 4. ğŸ¯ Launch Server

**A2A Protocol Server (Recommended):**
```bash
uv run src/server/app_a2a.py
```

> ğŸŒ Test at `http://localhost:8008/.well-known/agent-card.json` after server startup

**FastA2A Server (Simplified):**
```bash
uv run src/server/app_fasta2a.py
```

> ğŸŒ Test at `http://localhost:8008/docs` after server startup

---

## ğŸ“¡ A2A Client Usage Guide

### ğŸ§ª Test with Built-in CLI Client

Use the included CLI client for quick testing:

```bash
# ğŸ” Search mode evaluation
uv run src/client/a2a_client.py --agent-type search

# ğŸ“ Context mode evaluation
uv run src/client/a2a_client.py --agent-type context

# ğŸ“„ Use example context file
uv run src/client/a2a_client.py --agent-type context --use-example-context
```

### ğŸ’» Custom A2A Client Implementation

```python
import asyncio
from uuid import uuid4
import httpx
from a2a.client import A2AClient, A2ACardResolver
from a2a.types import Message, MessageSendParams, SendStreamingMessageRequest

class HallucinationCheckClient:
    def __init__(self, host="localhost", port=8008):
        self.base_url = f"http://{host}:{port}"
        
    async def check_with_search(self, statement):
        """ê²€ìƒ‰ ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì‚¬"""
        async with httpx.AsyncClient() as httpx_client:
            # A2A ì—ì´ì „íŠ¸ ì¹´ë“œ ê°€ì ¸ì˜¤ê¸°
            agent_card = await A2ACardResolver(
                httpx_client=httpx_client,
                base_url=self.base_url,
            ).get_agent_card()

            # A2A í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            client = A2AClient(
                httpx_client=httpx_client,
                agent_card=agent_card,
            )

            # ë©”ì‹œì§€ í˜ì´ë¡œë“œ êµ¬ì„±
            context_id = uuid4().hex
            send_message_payload = {
                "message": Message(
                    message_id=uuid4().hex,
                    context_id=context_id,
                    role="user",
                    parts=[{"type": "text", "text": statement}],
                ),
                "metadata": {
                    "enable_tavily_search_engine/v1": {"enable": True}
                }
            }

            # ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ìš”ì²­
            message_request = SendStreamingMessageRequest(
                id=uuid4().hex, 
                params=MessageSendParams(**send_message_payload)
            )

            # ì‘ë‹µ ì²˜ë¦¬
            async for chunk in client.send_message_streaming(message_request):
                chunk_data = chunk.model_dump(mode='json', exclude_none=True)
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if status := chunk_data.get("result", {}).get("status"):
                    print(f"Status: {status['state']}")
                    
                # ì™„ë£Œëœ ê²°ê³¼ ë°˜í™˜
                if chunk_data.get("result", {}).get("status", {}).get("state") == "completed":
                    return chunk_data["result"]

    async def check_with_context(self, statement, context):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì‚¬"""
        async with httpx.AsyncClient() as httpx_client:
            agent_card = await A2ACardResolver(
                httpx_client=httpx_client,
                base_url=self.base_url,
            ).get_agent_card()

            client = A2AClient(
                httpx_client=httpx_client,
                agent_card=agent_card,
            )

            context_id = uuid4().hex
            send_message_payload = {
                "message": Message(
                    message_id=uuid4().hex,
                    context_id=context_id,
                    role="user",
                    parts=[{"type": "text", "text": statement}],
                ),
                "metadata": {
                    "set_input_context_explicitly/v1": {"input_context": context}
                }
            }

            message_request = SendStreamingMessageRequest(
                id=uuid4().hex, 
                params=MessageSendParams(**send_message_payload)
            )

            async for chunk in client.send_message_streaming(message_request):
                chunk_data = chunk.model_dump(mode='json', exclude_none=True)
                if chunk_data.get("result", {}).get("status", {}).get("state") == "completed":
                    return chunk_data["result"]

# ğŸ“‹ Usage Example
async def main():
    client = HallucinationCheckClient()

    # ğŸ” Search-based evaluation
    print("=== ğŸ” Search-based Evaluation ===")
    result1 = await client.check_with_search("Tesla invented the light bulb")
    print(f"Result: {result1}")

    # ğŸ“ Context-based evaluation
    print("\n=== ğŸ“ Context-based Evaluation ===")
    context = "Tesla was known for AC electrical systems and wireless technology."
    result2 = await client.check_with_context("Tesla invented the light bulb", context)
    print(f"Result: {result2}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ“š References

* [SelfCheckAgent(25.02)](https://arxiv.org/html/2502.01812v1)
* [SelfCheckGPT(23.10)](https://github.com/potsawee/selfcheckgpt)
* [NLI ê¸°ë°˜ ë¬¸ì¥ ìê¸°ëª¨ìˆœ ë° í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ ë°©ë²• ìš”ì•½](assets/what_is_nli.md)

